# Incident Report

## Summary

### Incident Overview  
April 8, 2025, a simulated chaos monkey attack disrupted the `jwt-pizza-service`, causing failed pizza order attempts and triggering a Grafana alert. The issue was discovered by a developer via Grafana on-call email alerts and resolved through a targeted API call. The total impact lasted 19 minutes, during which five payment orders were lost. Though the incident occurred in a simulated environment, it highlights real-world issues surrounding observability, alerting thresholds, and third-party dependencies.

### Impact Overview  
The impact of this incident caused failed pizza order transactions, service downtime, and potential trust degradation from simulated users.

### Resolution Overview  
This issue was resolved by invoking a recovery API endpoint after identifying the source of failure through Grafana logs and alerts.

---

## Detection

This incident was initially detected by the developer through Grafana on-call email services. Although 3 of the fictitious customers in the traffic would have felt the errors before it made news to me, and potentially could have resulted in even faster detection.

Grafana sent the following email notifying of the pizza errors (clipping potentially sensitive links):

**Firing**  
**Value:** `C=1`  
**Labels:**
- `alertname = Pizza Errors`
- `grafana_folder = GrafanaCloud`
- `source = jwt-pizza-service`

**Annotations:** [Redacted links]

This email notification was followed up with a closer look at the problem using the error log panel on the Grafana dashboard to locate the chaos monkey.

Detection of error resolution was concluded with a success message after the recovery API, and a check to the logs to verify there were no more out-of-place error messages.

---

## Impact

The incident was relatively mitigated due to quick detection but was mildly extended due to another urgency discussed in the timeline. Overall, the errors caused by the chaos were up for 19 minutes, leaving a crucial aspect of the website down for that time.

Five pizza payment orders were failed and subsequently dropped by the simulated customers. Had these been real people, there likely would be customers trying to re-queue those orders and doubling or tripling those 5 errors.

Five orders might not be a lot for a larger business, but for a smaller organization like what JWT seems to be, it would be more significant. In the simulated traffic, 0.02 Bitcoins were lost, which is around ~$75,000 due to artificially high digital pizza markup—the monopoly has gone too far.

Another potential issue is company trust and word of mouth. In my view, the impact on trust would be minimal. Twenty minutes of downtime for payment isn’t severe, but when it comes to word of mouth, having 5 customers potentially not recommend it to their connections could hurt potential growth.

---

## Timeline

- **11:54:51.017 AM** — First chaos monkey message received  
- **12:13:38.997 PM** — Final chaos monkey message received (17 total)

- **11:57:30 AM** — First customer tried and failed to order pizza  
- 2 more failed before the alert reached me  
- 2 additional orders failed afterward  
- **Total failed orders: 5**

- **~12:03 PM** — Noticed the error  
- **~12:06 PM** — Began checking Grafana dashboard  
- **12:07:30 PM** — Temporarily shut off simulated traffic to ride the bus  
- **12:13 PM** — Turned traffic back on and called the recovery URL  
- **12:13:38 PM** — Final chaos monkey message confirmed

---

## Root Cause Analysis

The root cause of the issue was clearly the chaos monkey. The downstream effect was that when consumers would order pizza, it would get blocked and return 500-level errors.

Due to an oversight in trying to fix the problem as fast as possible, I didn’t perform much manual testing or probing of the issue, and it's no longer possible to recreate. Because of this, it's hard to understand the specifics of how it worked, but it likely looked at your API key and, if linked to the API key in the website, stopped production in the factory.

---

## Resolution

After looking through the logs, the issue was quickly resolved by manually calling the recovery endpoint specified in the chaos monkey’s message:

```
status: 500  
body: {
  "message": "chaos monkey",
  "reportUrl": "https://cs329.cs.byu.edu/api/report?apiKey=227c072583bb4370ad8c304134fecd59&fixCode=baca0a8fb42d42f3937d42e1045a3958"
}
```

**Response from the URL:**

```json
{
  "msg": "Chaos resolved"
}
```

Unfortunately, I didn’t record the details of the packets being transferred, and subsequent calls to the same endpoint resulted in `304 (Nothing new to send)` response codes.

Afterwards, the fix to the chaos monkey triggered my Grafana alert resolution. It displayed as the following:

**Resolved**  
**Value:** `C=0`  
**Labels:**
- `alertname = Pizza Errors`
- `grafana_folder = GrafanaCloud`
- `source = jwt-pizza-service`

**Annotations:** [Redacted links]

---

## Prevention

To prevent this from happening again:

1. **Tune detection thresholds**  
   My error log rates did increase, but due to misconfigured threshold values set too high, the alert didn’t trigger. I've now tuned these properly to match expected traffic behavior.

2. **Expand alert rules**  
   Added more alert rules to catch specific 500-level errors—or better yet, any non-404/503 errors (since those are expected under load).

3. **Engage third-party vendors**  
   I would investigate and negotiate with the third-party pizza factory that handles orders. Since JWT developers can’t change or secure their code, getting proactive updates or guarantees from them would be crucial to prevent actual hacking attempts.

---

## Action Items

The takeaway lesson is that **observability is critical**—and that it must be applied well. It’s not enough to simply collect metrics; they must be usable for real-world issues like outages or security threats.

In this case, the chaos monkey was:

1. Informed about beforehand  
2. Easy to detect  
3. Easy to fix  

Real-world problems will likely have none of those luxuries. This exercise highlights the need for robust metrics and alerts. 

One example: while reviewing metrics, I noticed what looked like a “hacking attempt”—repeated calls to endpoints like `/prod/.env`, likely scanning for vulnerabilities. Since I had no alerts for this behavior, it only came to light via manual log digging.

If a real attacker had been subtle, these events could have gone completely unnoticed without better monitoring and alerting.
